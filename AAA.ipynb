{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /vol/home/s4422090/.local/lib/python3.9/site-packages (2.13.0)\n",
      "Requirement already satisfied: keras in /vol/home/s4422090/.local/lib/python3.9/site-packages (2.13.1)\n",
      "Requirement already satisfied: numpy in /vol/home/s4422090/.local/lib/python3.9/site-packages (1.24.3)\n",
      "Requirement already satisfied: requests in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (2.32.3)\n",
      "Requirement already satisfied: scikit-learn in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (1.5.2)\n",
      "Requirement already satisfied: matplotlib in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (3.9.2)\n",
      "Collecting ConfigSpace\n",
      "  Using cached configspace-1.2.0.tar.gz (130 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /vol/home/s4422090/.local/lib/python3.9/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (from tensorflow) (1.67.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (from tensorflow) (4.25.5)\n",
      "Requirement already satisfied: setuptools in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /vol/home/s4422090/.local/lib/python3.9/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /vol/home/s4422090/.local/lib/python3.9/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /vol/home/s4422090/.local/lib/python3.9/site-packages (from tensorflow) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (from matplotlib) (6.4.5)\n",
      "Collecting more-itertools (from ConfigSpace)\n",
      "  Using cached more_itertools-10.5.0-py3-none-any.whl.metadata (36 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.20.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /vol/home/s4422090/.local/lib/python3.9/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.35.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /vol/home/s4422090/.local/lib/python3.9/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /vol/home/s4422090/.local/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /vol/home/s4422090/.local/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /vol/home/s4422090/.local/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /vol/home/s4422090/.local/lib/python3.9/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow) (8.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /vol/home/s4422090/miniconda3/envs/myenv/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /vol/home/s4422090/.local/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /vol/home/s4422090/.local/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n",
      "Using cached more_itertools-10.5.0-py3-none-any.whl (60 kB)\n",
      "Building wheels for collected packages: ConfigSpace\n",
      "  Building wheel for ConfigSpace (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ConfigSpace: filename=ConfigSpace-1.2.0-py3-none-any.whl size=115855 sha256=3ea14f70bb63c1e378e671ef23dabf63fce228c03be34680bca9cf0c7fc69a55\n",
      "  Stored in directory: /vol/home/s4422090/.cache/pip/wheels/e1/d9/f5/da696a970b97ddcd69be38c41ca81ab5d6634234b65d870d98\n",
      "Successfully built ConfigSpace\n",
      "Installing collected packages: more-itertools, ConfigSpace\n",
      "Successfully installed ConfigSpace-1.2.0 more-itertools-10.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow keras numpy requests scikit-learn matplotlib ConfigSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 0\n",
      "No GPU detected. Using CPU.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available:\", len(gpus))\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # Enable memory growth for each GPU to prevent TensorFlow from allocating all GPU memory at once\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(\"Error setting memory growth:\", e)\n",
    "else:\n",
    "    print(\"No GPU detected. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Grid Search on Fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ConfigSpace import ConfigurationSpace\n",
    "from ConfigSpace.hyperparameters import CategoricalHyperparameter, Constant\n",
    "from ConfigSpace.util import generate_grid\n",
    "\n",
    "\n",
    "def train_model(config):\n",
    "    # Extract hyperparameters from the configuration\n",
    "    filters = config['filters']\n",
    "    kernel_size = config['kernel_size']\n",
    "    activation = config['activation']\n",
    "    optimizer = config['optimizer']\n",
    "    batch_size = config['batch_size']\n",
    "    epochs = config['epochs']\n",
    "    kernel_initializer = config['kernel_initializer']\n",
    "    dropout_rate = config['dropout_rate']\n",
    "    max_pool_size = config['max_pool_size']\n",
    "    FC_size = config['FC_size']\n",
    "    \n",
    "\n",
    "    # Build the model\n",
    "    model = Sequential([\n",
    "        Conv2D(filters, kernel_size=(kernel_size, kernel_size), activation=activation, input_shape=input_shape, kernel_initializer=kernel_initializer),\n",
    "        Conv2D(filters * 2, kernel_size=(kernel_size, kernel_size), activation=activation, kernel_initializer=kernel_initializer),\n",
    "        MaxPooling2D(pool_size=(max_pool_size, max_pool_size)),\n",
    "        Dropout(dropout_rate),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(FC_size, activation=activation, kernel_initializer=kernel_initializer),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=optimizer,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        verbose=0,  \n",
    "        validation_data=(X_valid, y_valid)\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    val_loss, val_accuracy = model.evaluate(X_valid, y_valid, verbose=0)\n",
    "\n",
    "    return val_accuracy\n",
    "\n",
    "\n",
    "# Load the Fashion MNIST dataset\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Normalize the input data to [0, 1] range\n",
    "X_train_full = X_train_full.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Split the training set into training and validation sets\n",
    "val_ratio = 0.1\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=val_ratio, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# Reshape input data\n",
    "img_rows, img_cols = 28, 28\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "X_valid = X_valid.reshape(X_valid.shape[0], img_rows, img_cols, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# One-hot encoding of target labels\n",
    "num_classes = 10\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_valid = to_categorical(y_valid, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "# Define the hyperparameter space\n",
    "cs = ConfigurationSpace()\n",
    "\n",
    "# Hyperparameters\n",
    "kernel_initializer = CategoricalHyperparameter('kernel_initializer', ['random_normal', 'he_normal'])\n",
    "activation = CategoricalHyperparameter('activation', ['relu', 'sigmoid'])\n",
    "optimizer = CategoricalHyperparameter('optimizer', ['adam', 'sgd'])\n",
    "filters = CategoricalHyperparameter('filters', [28, 56])\n",
    "kernel_size = CategoricalHyperparameter('kernel_size', [3, 5])\n",
    "dropout_rate = CategoricalHyperparameter('dropout_rate', [0.0, 0.25, 0.5])\n",
    "max_pool_size = CategoricalHyperparameter('max_pool_size', [2,3])\n",
    "FC_size = CategoricalHyperparameter('FC_size', [128, 256])\n",
    "batch_size = Constant('batch_size', 128)\n",
    "epochs = Constant('epochs', 10)\n",
    "\n",
    "# Add hyperparameters to the configuration space\n",
    "cs.add_hyperparameters([filters, kernel_size, activation, optimizer, batch_size, epochs, kernel_initializer, dropout_rate, max_pool_size, FC_size])\n",
    "\n",
    "# Generate the grid of configurations\n",
    "grid = generate_grid(cs)\n",
    "print(f\"Total configurations to evaluate: {len(grid)}\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for idx, config in enumerate(grid):\n",
    "    config_dict = config.get_dictionary()\n",
    "    print(f\"Evaluating configuration {idx + 1}/{len(grid)}: {config_dict}\")\n",
    "    val_accuracy = train_model(config_dict)\n",
    "    print(f\"Validation accuracy: {val_accuracy:.4f}\\n\")\n",
    "\n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'config': config_dict,\n",
    "        'val_accuracy': val_accuracy\n",
    "    })\n",
    "\n",
    "# Sort the results by validation accuracy in descending order\n",
    "results = sorted(results, key=lambda x: x['val_accuracy'], reverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Grid Search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "df_results = pd.DataFrame(results)\n",
    "config_df = pd.json_normalize(df_results['config'])\n",
    "df = pd.concat([config_df, df_results['val_accuracy']], axis=1)\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"First first 10 rows of the DataFrame:\")\n",
    "display(df.head(10))\n",
    "\n",
    "\n",
    "# ANOVA Analysis\n",
    "\n",
    "df_anova = df.copy()\n",
    "\n",
    "# List of hyperparameters to include in the ANOVA table\n",
    "anova_hps = ['filters', 'kernel_size', 'kernel_initializer', 'activation', 'optimizer', 'dropout_rate', 'max_pool_size', 'FC_size']\n",
    "\n",
    "\n",
    "\n",
    "CAN I DELETE THIS? \n",
    "# Convert hyperparameters to categorical data types\n",
    "for col in anova_hps:\n",
    "    df_anova[col] = df_anova[col].astype('category')\n",
    "\n",
    "# Construct the formula for ANOVA\n",
    "formula = 'val_accuracy ~ ' + ' + '.join(anova_hps)\n",
    "\n",
    "# Perform ANOVA\n",
    "model = ols(formula, data=df_anova).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Round the 'PR(>F)' column to four decimal places\n",
    "anova_table['PR(>F)'] = anova_table['PR(>F)'].round(4)\n",
    "\n",
    "# Display the ANOVA table\n",
    "print(\"ANOVA Table:\")\n",
    "display(anova_table)\n",
    "\n",
    "\n",
    "# Average Validation Accuracy for Each Hyperparameter Value\n",
    "\n",
    "# List of hyperparameters to include in the table \n",
    "hyperparameters = ['filters', 'kernel_size', 'kernel_initializer', 'activation', 'optimizer', 'dropout_rate', 'max_pool_size', 'FC_size']\n",
    "\n",
    "\n",
    "mean_accuracy_list = []\n",
    "\n",
    "# Loop through each hyperparameter to calculate mean validation accuracy per value\n",
    "for hp in hyperparameters:\n",
    "    # Group by the hyperparameter and calculate the mean validation accuracy\n",
    "    hp_mean = df.groupby(hp)['val_accuracy'].mean().reset_index()\n",
    "    \n",
    "    # Rename the 'val_accuracy' column to 'Mean Val Accuracy'\n",
    "    hp_mean = hp_mean.rename(columns={'val_accuracy': 'Mean Val Accuracy', hp: 'Value'})\n",
    "    \n",
    "    # Add a column to indicate which hyperparameter the row corresponds to\n",
    "    hp_mean.insert(0, 'Hyperparameter', hp.replace('_', ' ').capitalize())\n",
    "    \n",
    "    # Append the result to the list\n",
    "    mean_accuracy_list.append(hp_mean)\n",
    "\n",
    "# Concatenate all the individual hyperparameter DataFrames into one\n",
    "mean_accuracy_df = pd.concat(mean_accuracy_list, ignore_index=True)\n",
    "\n",
    "# Round the mean validation accuracy to four decimals\n",
    "mean_accuracy_df['Mean Val Accuracy'] = mean_accuracy_df['Mean Val Accuracy'].round(4)\n",
    "\n",
    "# Display the table\n",
    "print(\"Average Validation Accuracy for Each Hyperparameter Value:\")\n",
    "display(mean_accuracy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and preprocess data\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "X_train_full = X_train_full.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "val_ratio = 0.1\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=val_ratio, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "img_rows, img_cols = 28, 28\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "X_valid = X_valid.reshape(X_valid.shape[0], img_rows, img_cols, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "num_classes = 10\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_valid = to_categorical(y_valid, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "# Function to build the model with a given number of blocks\n",
    "def build_model(num_blocks):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First Convolutional Layer\n",
    "    model.add(Conv2D(28, kernel_size=(3, 3), activation='relu', input_shape=input_shape, kernel_initializer='he_normal'))\n",
    "    model.add(Conv2D(28, kernel_size=(3, 3), activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "    for i in range(num_blocks):\n",
    "        filters = 28 * (2 ** i)\n",
    "\n",
    "        model.add(Conv2D(filters, kernel_size=(3, 3), activation='relu', kernel_initializer='he_normal'))\n",
    "        model.add(Conv2D(filters, kernel_size=(3, 3), activation='relu', kernel_initializer='he_normal'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))       \n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Loop through the number of blocks and evaluate the model\n",
    "validation_accuracies = []\n",
    "\n",
    "for num_blocks in range(1, 4):\n",
    "    print(f\"Training model with {num_blocks} block(s)...\")\n",
    "    model = build_model(num_blocks)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=128,\n",
    "        epochs=10,\n",
    "        verbose=0,\n",
    "        validation_data=(X_valid, y_valid)\n",
    "    )\n",
    "    score = model.evaluate(X_valid, y_valid, verbose=0)\n",
    "    validation_accuracies.append(score[1])\n",
    "    print(f\"Validation accuracy with {num_blocks} block(s): {score[1]:.4f}\")\n",
    "\n",
    "# Print the validation accuracies for each number of blocks\n",
    "for num_blocks, val_acc in enumerate(validation_accuracies, start=1):\n",
    "    print(f\"Validation accuracy with {num_blocks} block(s): {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and preprocess data\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "X_train_full = X_train_full.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "val_ratio = 0.1\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=val_ratio, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "img_rows, img_cols = 28, 28\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "X_valid = X_valid.reshape(X_valid.shape[0], img_rows, img_cols, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "num_classes = 10\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_valid = to_categorical(y_valid, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "# Function to build the model with a given number of blocks\n",
    "def build_model(num_blocks):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First Convolutional Layer\n",
    "    model.add(Conv2D(28, kernel_size=(3, 3), activation='relu', input_shape=input_shape, kernel_initializer='he_normal'))\n",
    "    model.add(Conv2D(28, kernel_size=(3, 3), activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "    for i in range(num_blocks):\n",
    "        filters = 28 * (2 ** i)\n",
    "\n",
    "        model.add(Conv2D(filters, kernel_size=(3, 3), activation='relu', kernel_initializer='he_normal'))\n",
    "        model.add(Conv2D(filters, kernel_size=(3, 3), activation='relu', kernel_initializer='he_normal'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))       \n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Loop through the number of blocks and evaluate the model\n",
    "validation_accuracies = []\n",
    "\n",
    "for num_blocks in range(1, 4):\n",
    "    print(f\"Training model with {num_blocks} block(s)...\")\n",
    "    model = build_model(num_blocks)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=128,\n",
    "        epochs=10,\n",
    "        verbose=0,\n",
    "        validation_data=(X_valid, y_valid)\n",
    "    )\n",
    "    score = model.evaluate(X_valid, y_valid, verbose=0)\n",
    "    validation_accuracies.append(score[1])\n",
    "    print(f\"Validation accuracy with {num_blocks} block(s): {score[1]:.4f}\")\n",
    "\n",
    "# Print the validation accuracies for each number of blocks\n",
    "for num_blocks, val_acc in enumerate(validation_accuracies, start=1):\n",
    "    print(f\"Validation accuracy with {num_blocks} block(s): {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 100 epochs on best Config, best number of conv blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR 3 - Top 3 best configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "X_train_full = X_train_full.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "val_ratio = 0.1\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=val_ratio, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# Define class names for reference \n",
    "class_names = [\n",
    "    'airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "    'dog', 'frog', 'horse', 'ship', 'truck'\n",
    "]\n",
    "\n",
    "#  Print dataset shapes \n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_valid.shape[0], 'validation samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# Set input shape\n",
    "img_rows, img_cols, img_channels = 32, 32, 3\n",
    "input_shape = (img_rows, img_cols, img_channels)\n",
    "\n",
    "# Flatten label arrays\n",
    "y_train = y_train.flatten()\n",
    "y_valid = y_valid.flatten()\n",
    "y_test = y_test.flatten()\n",
    "\n",
    "# One-hot encoding\n",
    "num_classes = 10  \n",
    "\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_valid = to_categorical(y_valid, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "kernel_initializer = 'he_normal'\n",
    "\n",
    "# 9. Define the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same', input_shape=input_shape, kernel_initializer=kernel_initializer),\n",
    "    Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same', kernel_initializer=kernel_initializer),\n",
    "    MaxPooling2D(pool_size=(3, 3)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu', kernel_initializer=kernel_initializer),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# 10. Compile the model\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',      # Suitable loss function for multi-class classification\n",
    "    optimizer='adam',                     # Adam optimizer\n",
    "    metrics=['accuracy']                  # Evaluate performance using accuracy\n",
    ")\n",
    "\n",
    "# 11. Train the model\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    "    verbose=0,\n",
    "    validation_data=(X_valid, y_valid)    # Use validation set for monitoring\n",
    ")\n",
    "\n",
    "# 12. Evaluate the model on the test set\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# config 2\n",
    "\n",
    "kernel_initializer = 'random_normal'\n",
    "\n",
    "# 9. Define the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same', input_shape=input_shape, kernel_initializer=kernel_initializer),\n",
    "    Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same', kernel_initializer=kernel_initializer),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu', kernel_initializer=kernel_initializer),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# 10. Compile the model\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',      # Suitable loss function for multi-class classification\n",
    "    optimizer='adam',                     # Adam optimizer\n",
    "    metrics=['accuracy']                  # Evaluate performance using accuracy\n",
    ")\n",
    "\n",
    "# 11. Train the model\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    "    verbose=0,\n",
    "    validation_data=(X_valid, y_valid)    # Use validation set for monitoring\n",
    ")\n",
    "\n",
    "# 12. Evaluate the model on the test set\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "\n",
    "# config 3\n",
    "\n",
    "kernel_initializer = 'random_normal'\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same', input_shape=input_shape, kernel_initializer=kernel_initializer),\n",
    "    Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same', kernel_initializer=kernel_initializer),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.0),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu', kernel_initializer=kernel_initializer),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',     \n",
    "    optimizer='adam',                     \n",
    "    metrics=['accuracy']                  \n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    "    verbose=0,\n",
    "    validation_data=(X_valid, y_valid)    \n",
    ")\n",
    "\n",
    "# 12. Evaluate the model on the test set\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR - test number of blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "# Verify TensorFlow installation and GPU availability\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available:\", len(gpus))\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # Enable memory growth for each GPU to prevent TensorFlow from allocating all GPU memory at once\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(\"Error setting memory growth:\", e)\n",
    "else:\n",
    "    print(\"No GPU detected. Using CPU.\")\n",
    "\n",
    "# 1. Load the CIFAR-10 dataset\n",
    "(X_train_full, y_train_full), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# 2. Normalize the input data to [0, 1] range\n",
    "X_train_full = X_train_full.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# 3. Split the full training set into training and validation sets\n",
    "val_ratio = 0.1\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=val_ratio, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# 4. Define class names for reference (optional)\n",
    "class_names = [\n",
    "    'airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "    'dog', 'frog', 'horse', 'ship', 'truck'\n",
    "]\n",
    "\n",
    "# 5. Print dataset shapes for verification\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_valid.shape[0], 'validation samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# 6. Set input shape based on CIFAR-10 images\n",
    "img_rows, img_cols, img_channels = 32, 32, 3\n",
    "input_shape = (img_rows, img_cols, img_channels)\n",
    "\n",
    "# 7. Flatten label arrays\n",
    "y_train = y_train.flatten()\n",
    "y_valid = y_valid.flatten()\n",
    "y_test = y_test.flatten()\n",
    "\n",
    "# 8. Convert class vectors to binary class matrices (one-hot encoding)\n",
    "num_classes = 10  # There are 10 classes in CIFAR-10\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_valid = to_categorical(y_valid, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "kernel_initializer = 'he_normal'\n",
    "\n",
    "# 9. Define the CNN model\n",
    "def build_model(num_blocks):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', input_shape=input_shape, kernel_initializer=kernel_initializer))\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', kernel_initializer=kernel_initializer))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    for i in range(1, num_blocks):\n",
    "        filters = 32 * (2 ** i)\n",
    "        model.add(Conv2D(filters, kernel_size=(3, 3), activation='relu', padding='same', kernel_initializer=kernel_initializer))\n",
    "        model.add(Conv2D(filters, kernel_size=(3, 3), activation='relu', padding='same', kernel_initializer=kernel_initializer))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu', kernel_initializer=kernel_initializer))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',    \n",
    "        optimizer='adam',                     \n",
    "        metrics=['accuracy']                 \n",
    "    )\n",
    "    return model\n",
    "\n",
    "validation_accuracies = []\n",
    "\n",
    "for num_blocks in range(1, 5):\n",
    "    print(f\"Training model with {num_blocks} block(s)...\")\n",
    "    model = build_model(num_blocks)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=128,\n",
    "        epochs=10,\n",
    "        verbose=0,\n",
    "        validation_data=(X_valid, y_valid)\n",
    "    )\n",
    "    score = model.evaluate(X_valid, y_valid, verbose=0)\n",
    "    validation_accuracies.append(score[1])\n",
    "    print(f\"Validation accuracy with {num_blocks} block(s): {score[1]:.4f}\")\n",
    "\n",
    "# Print the validation accuracies for each number of blocks\n",
    "for num_blocks, val_acc in enumerate(validation_accuracies, start=1):\n",
    "    print(f\"Validation accuracy with {num_blocks} block(s): {val_acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR - 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt  # Import for plotting\n",
    "\n",
    "# Verify TensorFlow installation and GPU availability\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available:\", len(gpus))\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # Enable memory growth for each GPU to prevent TensorFlow from allocating all GPU memory at once\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(\"Error setting memory growth:\", e)\n",
    "else:\n",
    "    print(\"No GPU detected. Using CPU.\")\n",
    "\n",
    "# 1. Load the CIFAR-10 dataset\n",
    "(X_train_full, y_train_full), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# 2. Normalize the input data to [0, 1] range\n",
    "X_train_full = X_train_full.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# 3. Split the full training set into training and validation sets\n",
    "val_ratio = 0.1\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=val_ratio, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# 4. Define class names for reference (optional)\n",
    "class_names = [\n",
    "    'airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "    'dog', 'frog', 'horse', 'ship', 'truck'\n",
    "]\n",
    "\n",
    "# 5. Print dataset shapes for verification\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_valid.shape[0], 'validation samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# 6. Set input shape based on CIFAR-10 images\n",
    "img_rows, img_cols, img_channels = 32, 32, 3\n",
    "input_shape = (img_rows, img_cols, img_channels)\n",
    "\n",
    "# 7. Flatten label arrays\n",
    "y_train = y_train.flatten()\n",
    "y_valid = y_valid.flatten()\n",
    "y_test = y_test.flatten()\n",
    "\n",
    "# 8. Convert class vectors to binary class matrices (one-hot encoding)\n",
    "num_classes = 10  # There are 10 classes in CIFAR-10\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_valid = to_categorical(y_valid, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "kernel_initializer = 'he_normal'\n",
    "\n",
    "# 9. Define the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', input_shape=input_shape, kernel_initializer=kernel_initializer),\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', kernel_initializer=kernel_initializer),\n",
    "    MaxPooling2D(pool_size=(3, 3)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "\n",
    "    Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same', kernel_initializer=kernel_initializer),\n",
    "    Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same', kernel_initializer=kernel_initializer),\n",
    "    MaxPooling2D(pool_size=(3, 3)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu', kernel_initializer=kernel_initializer),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# 10. Compile the model\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',      # Suitable loss function for multi-class classification\n",
    "    optimizer='adam',                     # Adam optimizer\n",
    "    metrics=['accuracy']                  # Evaluate performance using accuracy\n",
    ")\n",
    "\n",
    "# 11. Train the model and capture the history\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=128,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    validation_data=(X_valid, y_valid)    # Use validation set for monitoring\n",
    ")\n",
    "\n",
    "# 12. Evaluate the model on the test set\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# 13. Plot training & validation accuracy values\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy over 100 Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
