{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: 'tensorflow,': Expected end or semicolon (after name and no valid version specifier)\n",
      "    tensorflow,\n",
      "              ^\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow, pandas, matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\baroi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 71ms/step - accuracy: 0.1163 - loss: 2.2951 - val_accuracy: 0.2082 - val_loss: 2.2674\n",
      "Epoch 2/12\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 69ms/step - accuracy: 0.1871 - loss: 2.2625 - val_accuracy: 0.3493 - val_loss: 2.2276\n",
      "Epoch 3/12\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 68ms/step - accuracy: 0.2647 - loss: 2.2234 - val_accuracy: 0.4605 - val_loss: 2.1746\n",
      "Epoch 4/12\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 68ms/step - accuracy: 0.3453 - loss: 2.1693 - val_accuracy: 0.5414 - val_loss: 2.1007\n",
      "Epoch 5/12\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 68ms/step - accuracy: 0.4167 - loss: 2.0973 - val_accuracy: 0.6154 - val_loss: 1.9971\n",
      "Epoch 6/12\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 65ms/step - accuracy: 0.4849 - loss: 1.9951 - val_accuracy: 0.6755 - val_loss: 1.8577\n",
      "Epoch 7/12\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 67ms/step - accuracy: 0.5331 - loss: 1.8673 - val_accuracy: 0.7239 - val_loss: 1.6836\n",
      "Epoch 8/12\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 72ms/step - accuracy: 0.5740 - loss: 1.7096 - val_accuracy: 0.7559 - val_loss: 1.4868\n",
      "Epoch 9/12\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 70ms/step - accuracy: 0.6135 - loss: 1.5344 - val_accuracy: 0.7821 - val_loss: 1.2897\n",
      "Epoch 10/12\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 68ms/step - accuracy: 0.6381 - loss: 1.3819 - val_accuracy: 0.8012 - val_loss: 1.1136\n",
      "Epoch 11/12\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 70ms/step - accuracy: 0.6626 - loss: 1.2454 - val_accuracy: 0.8145 - val_loss: 0.9716\n",
      "Epoch 12/12\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 69ms/step - accuracy: 0.6813 - loss: 1.1328 - val_accuracy: 0.8286 - val_loss: 0.8609\n",
      "Test loss: 0.8608654141426086\n",
      "Test accuracy: 0.8285999894142151\n"
     ]
    }
   ],
   "source": [
    "'''Trains a simple convnet on the MNIST dataset.\n",
    "\n",
    "Gets to 99.25% test accuracy after 12 epochs\n",
    "(there is still a lot of margin for parameter tuning).\n",
    "16 seconds per epoch on a GRID K520 GPU.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (55000, 28, 28)\n",
      "55000 train samples\n",
      "10000 test samples\n",
      "Epoch 1/5\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 65ms/step - accuracy: 0.6843 - loss: 1.0451 - val_accuracy: 0.8800 - val_loss: 0.3375\n",
      "Epoch 2/5\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 66ms/step - accuracy: 0.8559 - loss: 0.4051 - val_accuracy: 0.8966 - val_loss: 0.2888\n",
      "Epoch 3/5\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 69ms/step - accuracy: 0.8743 - loss: 0.3440 - val_accuracy: 0.9102 - val_loss: 0.2567\n",
      "Epoch 4/5\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 66ms/step - accuracy: 0.8899 - loss: 0.3057 - val_accuracy: 0.9074 - val_loss: 0.2462\n",
      "Epoch 5/5\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 65ms/step - accuracy: 0.9001 - loss: 0.2743 - val_accuracy: 0.9208 - val_loss: 0.2234\n",
      "Test loss: 0.2518099546432495\n",
      "Test accuracy: 0.906499981880188\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# 1. Load the Fashion MNIST dataset\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# 2. Normalize the input data to [0, 1] range\n",
    "X_train_full = X_train_full.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# 3. Split the full training set into training and validation sets\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# 4. Define class names for reference (optional)\n",
    "class_names = [\n",
    "    \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "    \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"\n",
    "]\n",
    "\n",
    "# 5. Print dataset shapes for verification\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# 6. Reshape input data based on the Keras backend\n",
    "img_rows, img_cols = 28, 28  # Fashion MNIST images are 28x28 pixels\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    # If the backend is Theano\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "    X_valid = X_valid.reshape(X_valid.shape[0], 1, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    # If the backend is TensorFlow\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "    X_valid = X_valid.reshape(X_valid.shape[0], img_rows, img_cols, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# 7. Convert class vectors to binary class matrices (one-hot encoding)\n",
    "num_classes = 10  # There are 10 classes in Fashion MNIST\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_valid = to_categorical(y_valid, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "kernel_initializer='he_normal'\n",
    "\n",
    "# 8. Define the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape, kernel_initializer = kernel_initializer),\n",
    "    Conv2D(64, kernel_size=(3, 3), activation='relu', kernel_initializer = kernel_initializer),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),  # Prevents overfitting\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu', kernel_initializer = kernel_initializer),\n",
    "    Dropout(0.5),   # Further prevents overfitting\n",
    "    Dense(num_classes, activation='softmax')  # Output layer with softmax activation\n",
    "])\n",
    "\n",
    "# 9. Compile the model\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',      # Suitable loss function for multi-class classification\n",
    "    optimizer='adam',                     # Adam optimizer\n",
    "    metrics=['accuracy']                  # Evaluate performance using accuracy\n",
    ")\n",
    "\n",
    "# 10. Train the model\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=128,\n",
    "    epochs=5,\n",
    "    verbose=1,\n",
    "    validation_data=(X_valid, y_valid)    # Use validation set for monitoring\n",
    ")\n",
    "\n",
    "# 11. Evaluate the model on the test set\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Initialization\n",
    "Test loss: 0.2415064424276352\n",
    "Test accuracy: 0.9107000231742859\n",
    "\n",
    "He Initialization - first layer\n",
    "Test loss: 0.24019594490528107\n",
    "Test accuracy: 0.9122999906539917\n",
    "\n",
    "He initialization - all layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_data(images_path, labels_path, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, random_state=42):\n",
    "\n",
    "\n",
    "    images = np.load(images_path)\n",
    "    labels = np.load(labels_path)\n",
    "\n",
    "    # Normalize the images to [0,1]\n",
    "    images = images.astype('float32') / 255.0\n",
    "\n",
    "    num_samples, height, width = images.shape\n",
    "\n",
    "\n",
    "    # Suffle + separete into training, validation, test\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        images, labels, \n",
    "        test_size= (1 - train_ratio), \n",
    "        random_state=random_state, \n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, \n",
    "        test_size= val_ratio / (test_ratio + val_ratio), \n",
    "        random_state=random_state, \n",
    "        shuffle=True\n",
    "    )   \n",
    "    \"\"\"\n",
    "    print(images.shape)\n",
    "    print(f\"Training set shape: {X_train.shape}, {y_train.shape}\")\n",
    "    print(f\"Validation set shape: {X_val.shape}, {y_val.shape}\")\n",
    "    print(f\"Test set shape: {X_test.shape}, {y_test.shape}\")\n",
    "    \"\"\"\n",
    "    \n",
    "    return [X_train, y_train, X_val, y_val, X_test, y_test]\n",
    "\n",
    "def label_transformation(labels):\n",
    "    \n",
    "    hours = labels[:, 0]\n",
    "    minutes = labels[:, 1]\n",
    "    \n",
    "    # Base label\n",
    "    new_labels = 2*hours\n",
    "\n",
    "    # Increment label by 1 if minute > 30\n",
    "    new_labels += (minutes > 30).astype(int)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "def label_transformation_regression(labels):\n",
    "    \n",
    "    hours = labels[:, 0]\n",
    "    minutes = labels[:, 1]\n",
    "\n",
    "    new_labels = hours + minutes/60\n",
    "\n",
    "    return new_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "\n",
    "class Classification(object):\n",
    "    \n",
    "    def __init__(self, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "    def build_model(self):\n",
    "        \n",
    "\n",
    "        self.model = Sequential([\n",
    "            Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=self.input_shape),\n",
    "            Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "            MaxPooling2D(pool_size=(2, 2)),\n",
    "            Dropout(0.25),  # Prevents overfitting\n",
    "            Flatten(),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dropout(0.5),   # Further prevents overfitting\n",
    "            Dense(self.num_classes, activation='softmax')  # Output layer with softmax activation\n",
    "        ])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def compile_model(self):\n",
    "        self.model.compile(\n",
    "            loss='categorical_crossentropy',     \n",
    "            optimizer='adam',                    \n",
    "            metrics=['accuracy']               \n",
    "        )\n",
    "\n",
    "\n",
    "    def train_model(self, batch_size=128, epochs=5):\n",
    "        self.model.fit(\n",
    "            self.X_train, self.y_train,\n",
    "            batch_size=128,\n",
    "            epochs=5,\n",
    "            verbose=1,\n",
    "            validation_data=(self.X_val, self.y_val)    # Use validation set for monitoring\n",
    "        )\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        score = self.model.evaluate(self.X_test, self.y_test, verbose=0)\n",
    "        print('Test loss:', score[0])\n",
    "        print('Test accuracy:', score[1])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 787ms/step - accuracy: 0.0777 - loss: 2.6653 - val_accuracy: 0.0783 - val_loss: 2.5584\n",
      "Epoch 2/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 776ms/step - accuracy: 0.0840 - loss: 2.5670 - val_accuracy: 0.0811 - val_loss: 2.5478\n",
      "Epoch 3/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 745ms/step - accuracy: 0.0814 - loss: 2.5585 - val_accuracy: 0.0817 - val_loss: 2.5507\n",
      "Epoch 4/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 743ms/step - accuracy: 0.0842 - loss: 2.5540 - val_accuracy: 0.0783 - val_loss: 2.5500\n",
      "Epoch 5/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 734ms/step - accuracy: 0.0843 - loss: 2.5538 - val_accuracy: 0.0950 - val_loss: 2.5326\n",
      "Epoch 6/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 827ms/step - accuracy: 0.1123 - loss: 2.5013 - val_accuracy: 0.1806 - val_loss: 2.3180\n",
      "Epoch 7/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 737ms/step - accuracy: 0.2104 - loss: 2.2048 - val_accuracy: 0.3306 - val_loss: 1.8185\n",
      "Epoch 8/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 766ms/step - accuracy: 0.3637 - loss: 1.7219 - val_accuracy: 0.4944 - val_loss: 1.3449\n",
      "Epoch 9/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 797ms/step - accuracy: 0.5354 - loss: 1.2122 - val_accuracy: 0.6550 - val_loss: 0.9335\n",
      "Epoch 10/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 782ms/step - accuracy: 0.6819 - loss: 0.8133 - val_accuracy: 0.7178 - val_loss: 0.7335\n",
      "Epoch 11/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 736ms/step - accuracy: 0.7755 - loss: 0.5852 - val_accuracy: 0.7661 - val_loss: 0.6056\n",
      "Epoch 12/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 728ms/step - accuracy: 0.8369 - loss: 0.4406 - val_accuracy: 0.7789 - val_loss: 0.5899\n",
      "Epoch 13/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 724ms/step - accuracy: 0.8700 - loss: 0.3581 - val_accuracy: 0.8017 - val_loss: 0.5751\n",
      "Epoch 14/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 726ms/step - accuracy: 0.9016 - loss: 0.2685 - val_accuracy: 0.8189 - val_loss: 0.4979\n",
      "Epoch 15/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 727ms/step - accuracy: 0.9120 - loss: 0.2376 - val_accuracy: 0.8156 - val_loss: 0.5480\n",
      "Epoch 16/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 732ms/step - accuracy: 0.9283 - loss: 0.2026 - val_accuracy: 0.8322 - val_loss: 0.4937\n",
      "Epoch 17/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 735ms/step - accuracy: 0.9331 - loss: 0.1842 - val_accuracy: 0.8289 - val_loss: 0.5403\n",
      "Epoch 18/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 726ms/step - accuracy: 0.9380 - loss: 0.1707 - val_accuracy: 0.8306 - val_loss: 0.5430\n",
      "Epoch 19/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 770ms/step - accuracy: 0.9493 - loss: 0.1393 - val_accuracy: 0.8311 - val_loss: 0.5475\n",
      "Epoch 20/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 907ms/step - accuracy: 0.9498 - loss: 0.1470 - val_accuracy: 0.8383 - val_loss: 0.5422\n",
      "Test loss: 0.49093717336654663\n",
      "Test accuracy: 0.8538888692855835\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_and_split_data(images_path, labels_path, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, random_state=42):\n",
    "\n",
    "\n",
    "    images = np.load(images_path)\n",
    "    labels = np.load(labels_path)\n",
    "\n",
    "    # Normalize the images to [0,1]\n",
    "    images = images.astype('float32') / 255.0 * np.pi\n",
    "    images = np.sin(images)\n",
    "\n",
    "    num_samples, height, width = images.shape\n",
    "\n",
    "\n",
    "    # Suffle + separete into training, validation, test\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        images, labels, \n",
    "        test_size= (1 - train_ratio), \n",
    "        random_state=random_state, \n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, \n",
    "        test_size= val_ratio / (test_ratio + val_ratio), \n",
    "        random_state=random_state, \n",
    "        shuffle=True\n",
    "    )   \n",
    "    \"\"\"\n",
    "    print(images.shape)\n",
    "    print(f\"Training set shape: {X_train.shape}, {y_train.shape}\")\n",
    "    print(f\"Validation set shape: {X_val.shape}, {y_val.shape}\")\n",
    "    print(f\"Test set shape: {X_test.shape}, {y_test.shape}\")\n",
    "    \"\"\"\n",
    "    \n",
    "    return [X_train, y_train, X_val, y_val, X_test, y_test]\n",
    "\n",
    "def label_transformation(labels):\n",
    "    \n",
    "    hours = labels[:, 0]\n",
    "    minutes = labels[:, 1]\n",
    "    \n",
    "    # Base label\n",
    "    new_labels = 2*hours\n",
    "\n",
    "    # Increment label by 1 if minute > 30\n",
    "    new_labels += (minutes > 30).astype(int)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    images_path = r'C:\\Users\\baroi\\OneDrive\\Desktop\\Leiden University CS - AI\\Deep Learning\\Assignment 1\\images.npy'\n",
    "    labels_path = r'C:\\Users\\baroi\\OneDrive\\Desktop\\Leiden University CS - AI\\Deep Learning\\Assignment 1\\labels.npy'\n",
    "    \n",
    "    \n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = load_and_split_data(images_path=images_path, labels_path=labels_path)\n",
    "\n",
    "    y_test1 = label_transformation(y_test)\n",
    "    y_train1 = label_transformation(y_train)\n",
    "    y_val1 = label_transformation(y_val)\n",
    "\n",
    "    width, height = X_train.shape[1], X_train.shape[2]\n",
    "    input_shape = (width, height, 1)\n",
    "    num_classes = y_test1.shape[1]\n",
    "\n",
    "    model = Sequential([\n",
    "        Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n",
    "        Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "        Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),  # Prevents overfitting\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.25),   # Further prevents overfitting\n",
    "        Dense(num_classes, activation='softmax')  # Output layer with softmax activation\n",
    "    ])\n",
    "        \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',     \n",
    "        optimizer='adam',                    \n",
    "        metrics=['accuracy']               \n",
    "    )\n",
    "\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train1,\n",
    "        batch_size=128,\n",
    "        epochs=20,\n",
    "        verbose=1,\n",
    "        validation_data=(X_val, y_val1)   \n",
    "    )\n",
    "\n",
    "\n",
    "    # Evaluate the model\n",
    "    score = model.evaluate(X_test, y_test1, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "\n",
    "\n",
    "\n",
    "    # run a model with output is 720 neurons - classification\n",
    "    # run a model where output is 2 neurons - regression - only this\n",
    "\n",
    "    # provide accuracy as  loss function i s- common sense difference between predicted and target values \n",
    "    # objective - minimize this loss function\n",
    "\n",
    "    # 4 types of model - classification, regreesion, multi-head models, label transformation (optional)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test loss: 0.49093717336654663\n",
    "Test accuracy: 0.8538888692855835"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\baroi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(None,), output.shape=(None, 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[119], line 93\u001b[0m\n\u001b[0;32m     71\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[0;32m     72\u001b[0m     Conv2D(\u001b[38;5;241m32\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39minput_shape),\n\u001b[0;32m     73\u001b[0m     Conv2D(\u001b[38;5;241m64\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     83\u001b[0m     Dense(num_classes, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Output layer with softmax activation\u001b[39;00m\n\u001b[0;32m     84\u001b[0m ])\n\u001b[0;32m     86\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m     87\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,     \n\u001b[0;32m     88\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,                    \n\u001b[0;32m     89\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]               \n\u001b[0;32m     90\u001b[0m )\n\u001b[1;32m---> 93\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m   \u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m    102\u001b[0m score \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test1, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\baroi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\baroi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py:580\u001b[0m, in \u001b[0;36mcategorical_crossentropy\u001b[1;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments `target` and `output` must be at least rank 1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    578\u001b[0m     )\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape):\n\u001b[1;32m--> 580\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments `target` and `output` must have the same rank \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(ndim). Received: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    583\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    584\u001b[0m     )\n\u001b[0;32m    585\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape, output\u001b[38;5;241m.\u001b[39mshape):\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 \u001b[38;5;241m!=\u001b[39m e2:\n",
      "\u001b[1;31mValueError\u001b[0m: Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(None,), output.shape=(None, 24)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_and_split_data(images_path, labels_path, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, random_state=42):\n",
    "\n",
    "\n",
    "    images = np.load(images_path)\n",
    "    labels = np.load(labels_path)\n",
    "\n",
    "    # Normalize the images to [0,1]\n",
    "    images = images.astype('float32') / 255.0 \n",
    "\n",
    "    num_samples, height, width = images.shape\n",
    "\n",
    "    # Suffle + separete into training, validation, test\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        images, labels, \n",
    "        test_size= (1 - train_ratio), \n",
    "        random_state=random_state, \n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, \n",
    "        test_size= val_ratio / (test_ratio + val_ratio), \n",
    "        random_state=random_state, \n",
    "        shuffle=True\n",
    "    )   \n",
    "    \"\"\"\n",
    "    print(images.shape)\n",
    "    print(f\"Training set shape: {X_train.shape}, {y_train.shape}\")\n",
    "    print(f\"Validation set shape: {X_val.shape}, {y_val.shape}\")\n",
    "    print(f\"Test set shape: {X_test.shape}, {y_test.shape}\")\n",
    "    \"\"\"\n",
    "    \n",
    "    return [X_train, y_train, X_val, y_val, X_test, y_test]\n",
    "\n",
    "def label_transformation(labels):\n",
    "    \n",
    "    hours = labels[:, 0]\n",
    "    minutes = labels[:, 1]\n",
    "    \n",
    "    # Base label\n",
    "    new_labels = 60*hours\n",
    "    # Increment label by 1 if minute > 30\n",
    "    new_labels += minutes\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    images_path = r'C:\\Users\\baroi\\OneDrive\\Desktop\\Leiden University CS - AI\\Deep Learning\\Assignment 1\\images.npy'\n",
    "    labels_path = r'C:\\Users\\baroi\\OneDrive\\Desktop\\Leiden University CS - AI\\Deep Learning\\Assignment 1\\labels.npy'\n",
    "    \n",
    "\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = load_and_split_data(images_path=images_path, labels_path=labels_path)\n",
    "\n",
    "    y_test1 = label_transformation(y_test)\n",
    "    y_train1 = label_transformation(y_train)\n",
    "    y_val1 = label_transformation(y_val)\n",
    "\n",
    "    width, height = X_train.shape[1], X_train.shape[2]\n",
    "    input_shape = (width, height, 1)\n",
    "    number_classes = 720\n",
    "\n",
    "\n",
    "    model = Sequential([\n",
    "        Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n",
    "        Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "        Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),  # Prevents overfitting\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.25),   # Further prevents overfitting\n",
    "        Dense(num_classes, activation='softmax')  # Output layer with softmax activation\n",
    "    ])\n",
    "        \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',     \n",
    "        optimizer='adam',                    \n",
    "        metrics=['accuracy']               \n",
    "    )\n",
    "\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train1,\n",
    "        batch_size=128,\n",
    "        epochs=20,\n",
    "        verbose=1,\n",
    "        validation_data=(X_val, y_val1)   \n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    score = model.evaluate(X_test, y_test1, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "\n",
    "    # run a model with output is 720 neurons - classification\n",
    "    # run a model where output is 2 neurons - regression - only this\n",
    "\n",
    "    # provide accuracy as  loss function i s- common sense difference between predicted and target values \n",
    "    # objective - minimize this loss function\n",
    "\n",
    "    # 4 types of model - classification, regreesion, multi-head models, label transformation (optional)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ssh -o ProxyCommand='ssh -g -L 6000:localhost:6000 s4422090@sshgw.leidenuniv.nl -q -W U0065025:22' -g -L 6000:localhost:6000  s4422090@U0065025\n",
    "AWEb/?D_35hcHmS\n",
    "AWEb/?D_35hcHmS\n",
    "AWEb/?D_35hcHmS\n",
    "\n",
    "ssh -K U0065025\n",
    "\n",
    "jupyter-notebook --no-browser --port=6000\n",
    "\n",
    "ssh s4422090@sshgw.leidenuniv.nl\n",
    "\n",
    "ssh -o ProxyCommand='ssh -g -L 6000:localhost:6000 s4422090@sshgw.leidenuniv.nl -q -W U0032484:22' -g -L 6000:localhost:6000 s4422090@U0032484\n",
    "\n",
    "U0065003\n",
    "\n",
    "\n",
    "\n",
    "ssh -o ProxyCommand='ssh -g -L 8008:localhost:8008 s4422090@sshgw.leidenuniv.nl -q -W U0065025:22' -g -L 8008:localhost:8008  s4422090@U0065025\n",
    "AWEb/?D_35hcHmS\n",
    "AWEb/?D_35hcHmS\n",
    "AWEb/?D_35hcHmS\n",
    "\n",
    "ssh -K U0065025\n",
    "\n",
    "jupyter-notebook --no-browser --port=8008\n",
    "\n",
    "ssh s4422090@sshgw.leidenuniv.nl\n",
    "\n",
    "U0065003\n",
    "\n",
    "conda activate myenv\n",
    "\n",
    "\n",
    "\n",
    "ssh -o ProxyCommand='ssh -g -L 9824:localhost:9824 s4422090@sshgw.leidenuniv.nl -q -W U0065041:22' -g -L 9824:localhost:9824  s4422090@U0065041\n",
    "AWEb/?D_35hcHmS\n",
    "AWEb/?D_35hcHmS\n",
    "\n",
    "ssh -K U0065041\n",
    "\n",
    "ssh s4422090@sshgw.leidenuniv.nl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 810ms/step - loss: 23.4771 - mae: 3.8199 - val_loss: 11.9116 - val_mae: 2.9850\n",
      "Epoch 2/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 781ms/step - loss: 12.2597 - mae: 3.0173 - val_loss: 11.8311 - val_mae: 2.9756\n",
      "Epoch 3/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 769ms/step - loss: 12.3721 - mae: 3.0412 - val_loss: 11.8373 - val_mae: 2.9684\n",
      "Epoch 4/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 754ms/step - loss: 12.2380 - mae: 3.0235 - val_loss: 11.7443 - val_mae: 2.9632\n",
      "Epoch 5/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 760ms/step - loss: 12.2781 - mae: 3.0330 - val_loss: 12.0738 - val_mae: 2.9919\n",
      "Epoch 6/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 757ms/step - loss: 12.2749 - mae: 3.0226 - val_loss: 10.3522 - val_mae: 2.6999\n",
      "Epoch 7/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 775ms/step - loss: 10.8121 - mae: 2.7339 - val_loss: 9.1912 - val_mae: 2.5428\n",
      "Epoch 8/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 747ms/step - loss: 9.6558 - mae: 2.5467 - val_loss: 8.4953 - val_mae: 2.4314\n",
      "Epoch 9/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 752ms/step - loss: 8.6056 - mae: 2.3800 - val_loss: 7.5970 - val_mae: 2.2236\n",
      "Epoch 10/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 747ms/step - loss: 8.3218 - mae: 2.3304 - val_loss: 7.4678 - val_mae: 2.2452\n",
      "Epoch 11/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 743ms/step - loss: 7.5305 - mae: 2.1988 - val_loss: 6.3434 - val_mae: 1.9943\n",
      "Epoch 12/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 747ms/step - loss: 6.5939 - mae: 2.0191 - val_loss: 6.2454 - val_mae: 1.9870\n",
      "Epoch 13/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 746ms/step - loss: 6.1358 - mae: 1.9427 - val_loss: 6.1303 - val_mae: 1.8809\n",
      "Epoch 14/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 790ms/step - loss: 5.7292 - mae: 1.8532 - val_loss: 5.1439 - val_mae: 1.7543\n",
      "Epoch 15/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 773ms/step - loss: 5.1199 - mae: 1.7564 - val_loss: 4.7611 - val_mae: 1.7018\n",
      "Epoch 16/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 799ms/step - loss: 4.7059 - mae: 1.6676 - val_loss: 4.2628 - val_mae: 1.5632\n",
      "Epoch 17/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 894ms/step - loss: 4.0532 - mae: 1.5317 - val_loss: 4.3477 - val_mae: 1.5567\n",
      "Epoch 18/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 795ms/step - loss: 4.1013 - mae: 1.5335 - val_loss: 3.5508 - val_mae: 1.3936\n",
      "Epoch 19/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 788ms/step - loss: 3.5196 - mae: 1.4187 - val_loss: 3.7678 - val_mae: 1.4283\n",
      "Epoch 20/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 795ms/step - loss: 3.0176 - mae: 1.3042 - val_loss: 3.4694 - val_mae: 1.3481\n",
      "Test loss (MSE): 3.5880115032196045\n",
      "Test Mean Absolute Error (MAE): 1.3690413236618042\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_and_split_data(images_path, labels_path, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, random_state=42):\n",
    "    images = np.load(images_path)\n",
    "    labels = np.load(labels_path)\n",
    "\n",
    "    # Normalize the images to [0,1]\n",
    "    images = images.astype('float32') / 255.0 * np.pi\n",
    "    images = np.sin(images)\n",
    "\n",
    "    num_samples, height, width = images.shape\n",
    "\n",
    "    # Shuffle + separate into training, validation, test\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        images, labels, \n",
    "        test_size= (1 - train_ratio), \n",
    "        random_state=random_state, \n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, \n",
    "        test_size= val_ratio / (test_ratio + val_ratio), \n",
    "        random_state=random_state, \n",
    "        shuffle=True\n",
    "    )   \n",
    "    \n",
    "    return [X_train, y_train, X_val, y_val, X_test, y_test]\n",
    "\n",
    "def label_transformation(labels):\n",
    "    # Convert labels from [\"03:00\"] to continuous [3.0]\n",
    "    hours = labels[:, 0].astype(float)\n",
    "    minutes = labels[:, 1].astype(float)\n",
    "\n",
    "    # Convert time to continuous value: hour + minutes/60\n",
    "    continuous_labels = hours + minutes / 60.0\n",
    "    return continuous_labels\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    images_path = r'C:\\Users\\baroi\\OneDrive\\Desktop\\Leiden University CS - AI\\Deep Learning\\Assignment 1\\images.npy'\n",
    "    labels_path = r'C:\\Users\\baroi\\OneDrive\\Desktop\\Leiden University CS - AI\\Deep Learning\\Assignment 1\\labels.npy'\n",
    "    \n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = load_and_split_data(images_path=images_path, labels_path=labels_path)\n",
    "\n",
    "    # Transform the labels to continuous values\n",
    "    y_test_cont = label_transformation(y_test)\n",
    "    y_train_cont = label_transformation(y_train)\n",
    "    y_val_cont = label_transformation(y_val)\n",
    "\n",
    "    # Reshape the labels to be compatible with the model (add an extra dimension)\n",
    "    y_test_cont = y_test_cont.reshape(-1, 1)\n",
    "    y_train_cont = y_train_cont.reshape(-1, 1)\n",
    "    y_val_cont = y_val_cont.reshape(-1, 1)\n",
    "\n",
    "    width, height = X_train.shape[1], X_train.shape[2]\n",
    "    input_shape = (width, height, 1)\n",
    "\n",
    "    model = Sequential([\n",
    "        Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n",
    "        Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "        Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),  # Prevents overfitting\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.25),   # Further prevents overfitting\n",
    "        Dense(1, activation='linear')  # Output layer for regression, single continuous output\n",
    "    ])\n",
    "        \n",
    "    model.compile(\n",
    "        loss='mean_squared_error',  # MSE for regression tasks !!! make loss function as sin\n",
    "        optimizer='adam',                    \n",
    "        metrics=['mae']  # Mean Absolute Error as an additional metric\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train_cont,\n",
    "        batch_size=128,\n",
    "        epochs=20,\n",
    "        verbose=1,\n",
    "        validation_data=(X_val, y_val_cont)   \n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    score = model.evaluate(X_test, y_test_cont, verbose=0)\n",
    "    print('Test loss (MSE):', score[0])\n",
    "    print('Test Mean Absolute Error (MAE):', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_and_split_data(images_path, labels_path, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, random_state=42):\n",
    "    images = np.load(images_path)\n",
    "    labels = np.load(labels_path)\n",
    "\n",
    "    # Normalize the images to [0,1]\n",
    "    images = images.astype('float32') / 255.0 * np.pi\n",
    "    images = np.sin(images)\n",
    "\n",
    "    num_samples, height, width = images.shape\n",
    "\n",
    "    # Shuffle + separate into training, validation, test\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        images, labels, \n",
    "        test_size= (1 - train_ratio), \n",
    "        random_state=random_state, \n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, \n",
    "        test_size= test_ratio / (test_ratio + val_ratio), \n",
    "        random_state=random_state, \n",
    "        shuffle=True\n",
    "    )   \n",
    "    \n",
    "    return [X_train, y_train, X_val, y_val, X_test, y_test]\n",
    "\n",
    "def label_transformation(labels):\n",
    "    # Extract hours and minutes\n",
    "    hours = labels[:, 0].astype(int)\n",
    "    minutes = labels[:, 1].astype(float)\n",
    "    return hours, minutes\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    images_path = r'C:\\Users\\baroi\\OneDrive\\Desktop\\Leiden University CS - AI\\Deep Learning\\Assignment 1\\images.npy'\n",
    "    labels_path = r'C:\\Users\\baroi\\OneDrive\\Desktop\\Leiden University CS - AI\\Deep Learning\\Assignment 1\\labels.npy'\n",
    "    \n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = load_and_split_data(images_path=images_path, labels_path=labels_path)\n",
    "\n",
    "    # Transform the labels to get hours and minutes\n",
    "    y_train_hours, y_train_minutes = label_transformation(y_train)\n",
    "    y_val_hours, y_val_minutes = label_transformation(y_val)\n",
    "    y_test_hours, y_test_minutes = label_transformation(y_test)\n",
    "\n",
    "    # Normalize minutes to range [0, 1]\n",
    "    y_train_minutes = y_train_minutes / 59.0\n",
    "    y_val_minutes = y_val_minutes / 59.0\n",
    "    y_test_minutes = y_test_minutes / 59.0\n",
    "\n",
    "    # Reshape images to include channel dimension\n",
    "    height, width = X_train.shape[1], X_train.shape[2]\n",
    "    X_train = X_train.reshape(-1, height, width, 1)\n",
    "    X_val = X_val.reshape(-1, height, width, 1)\n",
    "    X_test = X_test.reshape(-1, height, width, 1)\n",
    "\n",
    "    input_shape = (height, width, 1)\n",
    "\n",
    "    # Build the model using functional API\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(32, kernel_size=(3, 3), activation='relu')(inputs)\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    # Output head for hours (classification)\n",
    "    hours_output = Dense(24, activation='softmax', name='hours_output')(x)\n",
    "\n",
    "    # Output head for minutes (regression)\n",
    "    minutes_output = Dense(1, activation='linear', name='minutes_output')(x)\n",
    "\n",
    "    # Define the model\n",
    "    model = Model(inputs=inputs, outputs=[hours_output, minutes_output])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss={'hours_output': 'sparse_categorical_crossentropy', 'minutes_output': 'mean_squared_error'},\n",
    "        metrics={'hours_output': 'accuracy', 'minutes_output': 'mae'}\n",
    "    )\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(\n",
    "        X_train, {'hours_output': y_train_hours, 'minutes_output': y_train_minutes},\n",
    "        batch_size=128,\n",
    "        epochs=20,\n",
    "        verbose=1,\n",
    "        validation_data=(X_val, {'hours_output': y_val_hours, 'minutes_output': y_val_minutes})\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluation = model.evaluate(X_test, {'hours_output': y_test_hours, 'minutes_output': y_test_minutes}, verbose=0)\n",
    "    for idx, metric_name in enumerate(model.metrics_names):\n",
    "        print(f'{metric_name}: {evaluation[idx]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
